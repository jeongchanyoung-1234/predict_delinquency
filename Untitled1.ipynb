{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import random\n",
    "\n",
    "train = pd.read_csv('data/train.csv')\n",
    "train = train.drop(['index'], axis=1)\n",
    "train.fillna('NAN', inplace=True) \n",
    "\n",
    "\n",
    "test = pd.read_csv('data/test.csv')\n",
    "test = test.drop(['index'], axis=1)\n",
    "test.fillna('NAN', inplace=True)\n",
    "\n",
    "submit = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "object_col = []\n",
    "for col in train.columns:\n",
    "    if train[col].dtype == 'object':\n",
    "        object_col.append(col)\n",
    "        \n",
    "enc = OneHotEncoder()\n",
    "enc.fit(train.loc[:,object_col])\n",
    "\n",
    "\n",
    "train_onehot_df = pd.DataFrame(enc.transform(train.loc[:,object_col]).toarray(), \n",
    "             columns=enc.get_feature_names(object_col))\n",
    "train.drop(object_col, axis=1, inplace=True)\n",
    "train = pd.concat([train, train_onehot_df], axis=1)\n",
    "\n",
    "test_onehot_df = pd.DataFrame(enc.transform(test.loc[:,object_col]).toarray(), \n",
    "             columns=enc.get_feature_names(object_col))\n",
    "test.drop(object_col, axis=1, inplace=True)\n",
    "test = pd.concat([test, test_onehot_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 'c:/Users/JCY/Dacon/shinhan/'\n",
    "imp=False\n",
    "def get_data(base, imp=True, ae=False):\n",
    "    import os\n",
    "    from copy import deepcopy\n",
    "    from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "    train_df = pd.read_csv(os.path.join(base, 'data/train.csv'))\n",
    "    test_df = pd.read_csv(os.path.join(base, 'data/test.csv'))\n",
    "    train_df_target_removed = train_df.drop(columns=['credit'])\n",
    "    train_df_target = train_df.iloc[:, -1]\n",
    "\n",
    "    df = pd.concat([train_df_target_removed, test_df], axis=0)\n",
    "\n",
    "    def birth2age(x):\n",
    "        return x*(-1) / 365\n",
    "    df['age'] = df['DAYS_BIRTH'].apply(birth2age)\n",
    "    df['skill'] = df['DAYS_EMPLOYED'].apply(birth2age)\n",
    "    df['month'] = df['begin_month'].apply(lambda x: x * (-1))\n",
    "\n",
    "    df['income_c'] = ''\n",
    "    df.loc[df['income_total'] <= 1.575e6, 'income_c'] = 'first'\n",
    "    df.loc[df['income_total'] <= 2.25e5, 'income_c'] ='second'\n",
    "    df.loc[df['income_total'] <= 1.575e5, 'income_c'] = 'third'\n",
    "    df.loc[df['income_total'] <= 1.215e5, 'income_c'] = 'fourth'\n",
    "\n",
    "    if imp:\n",
    "        df.loc[df['occyp_type'].isnull() &\\\n",
    "               (df['income_type'] == 'Commercial associate') &\\\n",
    "               (df['gender'] == 'F'), 'occyp_type'] = 'Sales staff'\n",
    "        df.loc[df['occyp_type'].isnull() &\\\n",
    "               (df['income_type'] == 'Commercial associate') &\\\n",
    "               (df['gender'] == 'F') &\\\n",
    "               (df['income_c'] == 'first'), 'occyp_type'] = 'Managers'\n",
    "        df.loc[df['occyp_type'].isnull() &\\\n",
    "               (df['income_type'] == 'Commercial associate') &\\\n",
    "               (df['gender'] == 'F') &\\\n",
    "               (df['income_c'] == 'third') &\\\n",
    "               (df['work_phone'] == 1), 'occyp_type'] = 'Core staff'\n",
    "        df.loc[df['occyp_type'].isnull() &\\\n",
    "               (df['income_type'] == 'Commercial associate') &\\\n",
    "               (df['gender'] == 'M'), 'occyp_type'] = 'Drivers'\n",
    "        df.loc[df['occyp_type'].isnull() &\\\n",
    "               (df['income_type'] == 'Commercial associate') &\\\n",
    "               (df['gender'] == 'M') &\\\n",
    "               (df['income_c'] == 'first'), 'occyp_type'] = 'Managers'\n",
    "        df.loc[df['occyp_type'].isnull() &\\\n",
    "               (df['income_type'] == 'Commercial associate') &\\\n",
    "               (df['gender'] == 'M') &\\\n",
    "               (df['income_c'] == 'fourth') &\\\n",
    "               (df['work_phone'] == 0), 'occyp_type'] = 'Laborers'\n",
    "        df.loc[df['occyp_type'].isnull() &\\\n",
    "               (df['income_type'] == 'Commercial associate') &\\\n",
    "               (df['gender'] == 'M') &\\\n",
    "               (df['income_c'] == 'fourth') &\\\n",
    "               (df['work_phone'] == 1), 'occyp_type'] = 'Drivers'\n",
    "        df.loc[df['occyp_type'].isnull() &\\\n",
    "               (df['income_type'] == 'Pensioner') &\\\n",
    "               (df['gender'] == 'F') &\\\n",
    "               (df['income_c'] == 'first') &\\\n",
    "               (df['work_phone'] == 0), 'occyp_type'] = 'Core staff'\n",
    "        df.loc[df['occyp_type'].isnull() &\\\n",
    "               (df['income_type'] == 'State servant'), 'occyp_type'] = 'Core staff'\n",
    "        df.loc[df['occyp_type'].isnull() &\\\n",
    "               (df['income_type'] == 'State servant') &\\\n",
    "               (df['gender'] == 'F') &\\\n",
    "               (df['income_c'] == 'first') &\\\n",
    "               (df['work_phone'] == 1), 'occyp_type'] = 'Managers'\n",
    "        df.loc[df['occyp_type'].isnull() &\\\n",
    "               (df['income_type'] == 'State servant') &\\\n",
    "               (df['gender'] == 'F') &\\\n",
    "               (df['income_c'] == 'second') &\\\n",
    "               (df['work_phone'] == 0), 'occyp_type'] = 'Medicine staff'\n",
    "        df.loc[df['occyp_type'].isnull() &\\\n",
    "               (df['income_type'] == 'State servant') &\\\n",
    "               (df['gender'] == 'M') &\\\n",
    "               (df['income_c'] == 'first') &\\\n",
    "               (df['work_phone'] == 1), 'occyp_type'] = 'High skill tech staff'\n",
    "        df.loc[df['occyp_type'].isnull() &\\\n",
    "               (df['income_type'] == 'State servant') &\\\n",
    "               (df['gender'] == 'M') &\\\n",
    "               (df['income_c'] == 'fouth') &\\\n",
    "               (df['work_phone'] == 0), 'occyp_type'] = 'Laborers'\n",
    "        df.loc[df['occyp_type'].isnull() &\\\n",
    "               (df['income_type'] == 'State servant') &\\\n",
    "               (df['gender'] == 'M') &\\\n",
    "               (df['income_c'] == 'third') &\\\n",
    "               (df['work_phone'] == 1), 'occyp_type'] = 'Drivers'\n",
    "        df.loc[df['occyp_type'].isnull() &\\\n",
    "               (df['income_type'] == 'Working'), 'occyp_type'] = 'Laborers'\n",
    "        df.loc[df['occyp_type'].isnull() &\\\n",
    "               (df['income_type'] == 'Working') &\\\n",
    "               (df['gender'] == 'F'), 'occyp_type'] = 'Sales staff'\n",
    "        df.loc[df['occyp_type'].isnull() &\\\n",
    "               (df['income_type'] == 'State servant') &\\\n",
    "               (df['gender'] == 'F') &\\\n",
    "               (df['income_c'] == 'first') &\\\n",
    "               (df['work_phone'] == 1), 'occyp_type'] = 'Managers'\n",
    "        df.loc[df['occyp_type'].isnull() &\\\n",
    "               (df['income_type'] == 'State servant') &\\\n",
    "               (df['gender'] == 'F') &\\\n",
    "               (df['income_c'] == 'second') &\\\n",
    "               (df['work_phone'] == 1), 'occyp_type'] = 'Core staff'\n",
    "        df.loc[df['occyp_type'].isnull() &\\\n",
    "               (df['income_type'] == 'Student') &\\\n",
    "               (df['income_c'] == 'fourth'), 'occyp_type'] = 'Laborers'\n",
    "        df.loc[df['occyp_type'].isnull() &\\\n",
    "               (df['income_type'] == 'Student') &\\\n",
    "               (df['income_c'] == 'second'), 'occyp_type'] = 'Core staff'\n",
    "        df.loc[df['occyp_type'].isnull() &\\\n",
    "               (df['income_type'] == 'Pensioner'), 'occyp_type'] = 'Laborers'\n",
    "    else:\n",
    "        df.drop(columns=['occyp_type'], inplace=True)\n",
    "\n",
    "    df['DAYS_BIRTH_month']=np.floor((-df['DAYS_BIRTH'])/30)-((np.floor((-df['DAYS_BIRTH'])/30)/12).astype(int)*12)\n",
    "    df['DAYS_BIRTH_week']=np.floor((-df['DAYS_BIRTH'])/7)-((np.floor((-df['DAYS_BIRTH'])/7)/4).astype(int)*4)\n",
    "\n",
    "\n",
    "    # DAYS_EMPLOYED\n",
    "    df['DAYS_EMPLOYED_month']=np.floor((-df['DAYS_EMPLOYED'])/30)-((np.floor((-df['DAYS_EMPLOYED'])/30)/12).astype(int)*12)\n",
    "    df['DAYS_EMPLOYED_week']=np.floor((-df['DAYS_EMPLOYED'])/7)-((np.floor((-df['DAYS_EMPLOYED'])/7)/4).astype(int)*4)\n",
    "\n",
    "    # before_EMPLOYED\n",
    "    df['before_EMPLOYED']=df['DAYS_BIRTH']-df['DAYS_EMPLOYED']\n",
    "    df['before_EMPLOYED_month']=np.floor((-df['before_EMPLOYED'])/30)-((np.floor((-df['before_EMPLOYED'])/30)/12).astype(int)*12)\n",
    "    df['before_EMPLOYED_week']=np.floor((-df['before_EMPLOYED'])/7)-((np.floor((-df['before_EMPLOYED'])/7)/4).astype(int)*4)\n",
    "\n",
    "    df.drop(columns=['begin_month', 'DAYS_EMPLOYED', 'DAYS_BIRTH', 'income_c'], inplace=True)\n",
    "    \n",
    "    object_col = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            object_col.append(col)\n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(df.loc[:,object_col])\n",
    "\n",
    "    onehot_df = pd.DataFrame(enc.transform(df.loc[:,object_col]).toarray(), \n",
    "                 columns=enc.get_feature_names(object_col))\n",
    "\n",
    "    new_df = pd.concat([df.reset_index(drop=True), onehot_df], axis=1)\n",
    "    new_df.drop(columns=object_col, inplace=True)\n",
    "    \n",
    "    train_df = new_df[:26457]\n",
    "    train_df = pd.concat([train_df, train_df_target], axis=1)  \n",
    "    test_df = new_df[26457:]\n",
    "\n",
    "    train = train_df.drop(columns=['index'])\n",
    "    test = test_df.drop(columns=['index'])\n",
    "    enc_list = enc.get_feature_names(object_col)\n",
    "    \n",
    "    if ae == True:\n",
    "        enc_result = pd.read_csv('./data/encode_result.csv')\n",
    "        train = pd.concat([train.drop(columns=enc_list).reset_index(drop=True),\n",
    "                           enc_result[:26457].reset_index(drop=True)],\n",
    "                          axis=1).reset_index(drop=True)\n",
    "        test = pd.concat([test.drop(columns=enc_list).reset_index(drop=True),\n",
    "                          enc_result[26457:].reset_index(drop=True)],\n",
    "                         axis=1).reset_index(drop=True)\n",
    "    \n",
    "        print('{} features are reduced to {}-d'.format(len(enc.get_feature_names(object_col)),\n",
    "                                                       len(enc_result.columns)))\n",
    "    \n",
    "    print('Shape: train {} test {} total {}'.format(train.shape, test.shape, new_df.shape))\n",
    "    \n",
    "    return train, train_df_target, test, new_df, enc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 features are reduced to 10-d\n",
      "Shape: train (26457, 28) test (10000, 27) total (36457, 64)\n"
     ]
    }
   ],
   "source": [
    "base = 'c:/Users/JCY/Dacon/shinhan/'\n",
    "imp = True\n",
    "train, train_df_target, test, new_df, enc_list = get_data(base, imp=imp, ae=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 125 candidates, totalling 625 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'num_leaves' : [90, 80, 70, 60, 50],\n",
    "    'min_data_in_leaf': [19, 21, 23, 25, 27],\n",
    "    'max_depth' : [15, 16, 17, 18, 19]\n",
    "}\n",
    "gs = GridSearchCV(lgb, params, scoring='neg_log_loss', n_jobs=-1, cv=5, verbose=1)\n",
    "gs.fit(train.drop(['credit'],axis=1), train['credit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================1============================================\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttraining's multi_logloss: 0.745573\tvalid_1's multi_logloss: 0.791134\n",
      "[200]\ttraining's multi_logloss: 0.674979\tvalid_1's multi_logloss: 0.755146\n",
      "[300]\ttraining's multi_logloss: 0.625712\tvalid_1's multi_logloss: 0.734641\n",
      "[400]\ttraining's multi_logloss: 0.58757\tvalid_1's multi_logloss: 0.721271\n",
      "[500]\ttraining's multi_logloss: 0.556075\tvalid_1's multi_logloss: 0.712042\n",
      "[600]\ttraining's multi_logloss: 0.528762\tvalid_1's multi_logloss: 0.705083\n",
      "[700]\ttraining's multi_logloss: 0.504919\tvalid_1's multi_logloss: 0.699825\n",
      "[800]\ttraining's multi_logloss: 0.483046\tvalid_1's multi_logloss: 0.695485\n",
      "[900]\ttraining's multi_logloss: 0.462971\tvalid_1's multi_logloss: 0.692462\n",
      "[1000]\ttraining's multi_logloss: 0.444848\tvalid_1's multi_logloss: 0.690191\n",
      "[1100]\ttraining's multi_logloss: 0.427987\tvalid_1's multi_logloss: 0.688768\n",
      "[1200]\ttraining's multi_logloss: 0.412365\tvalid_1's multi_logloss: 0.687655\n",
      "Early stopping, best iteration is:\n",
      "[1206]\ttraining's multi_logloss: 0.411448\tvalid_1's multi_logloss: 0.687622\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================2============================================\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttraining's multi_logloss: 0.743758\tvalid_1's multi_logloss: 0.795775\n",
      "[200]\ttraining's multi_logloss: 0.672516\tvalid_1's multi_logloss: 0.762732\n",
      "[300]\ttraining's multi_logloss: 0.623003\tvalid_1's multi_logloss: 0.744672\n",
      "[400]\ttraining's multi_logloss: 0.584658\tvalid_1's multi_logloss: 0.732915\n",
      "[500]\ttraining's multi_logloss: 0.552832\tvalid_1's multi_logloss: 0.724764\n",
      "[600]\ttraining's multi_logloss: 0.52525\tvalid_1's multi_logloss: 0.718851\n",
      "[700]\ttraining's multi_logloss: 0.500916\tvalid_1's multi_logloss: 0.714472\n",
      "[800]\ttraining's multi_logloss: 0.479029\tvalid_1's multi_logloss: 0.711541\n",
      "[900]\ttraining's multi_logloss: 0.45938\tvalid_1's multi_logloss: 0.709529\n",
      "[1000]\ttraining's multi_logloss: 0.441129\tvalid_1's multi_logloss: 0.70841\n",
      "Early stopping, best iteration is:\n",
      "[1036]\ttraining's multi_logloss: 0.434938\tvalid_1's multi_logloss: 0.708098\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================3============================================\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttraining's multi_logloss: 0.744777\tvalid_1's multi_logloss: 0.794291\n",
      "[200]\ttraining's multi_logloss: 0.673098\tvalid_1's multi_logloss: 0.760916\n",
      "[300]\ttraining's multi_logloss: 0.623185\tvalid_1's multi_logloss: 0.742922\n",
      "[400]\ttraining's multi_logloss: 0.584339\tvalid_1's multi_logloss: 0.731436\n",
      "[500]\ttraining's multi_logloss: 0.552488\tvalid_1's multi_logloss: 0.723713\n",
      "[600]\ttraining's multi_logloss: 0.524965\tvalid_1's multi_logloss: 0.717882\n",
      "[700]\ttraining's multi_logloss: 0.500662\tvalid_1's multi_logloss: 0.713245\n",
      "[800]\ttraining's multi_logloss: 0.478519\tvalid_1's multi_logloss: 0.710699\n",
      "[900]\ttraining's multi_logloss: 0.458425\tvalid_1's multi_logloss: 0.708946\n",
      "[1000]\ttraining's multi_logloss: 0.440153\tvalid_1's multi_logloss: 0.708136\n",
      "Early stopping, best iteration is:\n",
      "[1007]\ttraining's multi_logloss: 0.438932\tvalid_1's multi_logloss: 0.70802\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================4============================================\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttraining's multi_logloss: 0.744924\tvalid_1's multi_logloss: 0.794003\n",
      "[200]\ttraining's multi_logloss: 0.673706\tvalid_1's multi_logloss: 0.759566\n",
      "[300]\ttraining's multi_logloss: 0.624404\tvalid_1's multi_logloss: 0.740357\n",
      "[400]\ttraining's multi_logloss: 0.58656\tvalid_1's multi_logloss: 0.728206\n",
      "[500]\ttraining's multi_logloss: 0.555025\tvalid_1's multi_logloss: 0.719588\n",
      "[600]\ttraining's multi_logloss: 0.527562\tvalid_1's multi_logloss: 0.713181\n",
      "[700]\ttraining's multi_logloss: 0.503316\tvalid_1's multi_logloss: 0.708094\n",
      "[800]\ttraining's multi_logloss: 0.481121\tvalid_1's multi_logloss: 0.704497\n",
      "[900]\ttraining's multi_logloss: 0.460934\tvalid_1's multi_logloss: 0.702262\n",
      "[1000]\ttraining's multi_logloss: 0.442798\tvalid_1's multi_logloss: 0.700686\n",
      "[1100]\ttraining's multi_logloss: 0.425721\tvalid_1's multi_logloss: 0.699973\n",
      "[1200]\ttraining's multi_logloss: 0.410007\tvalid_1's multi_logloss: 0.699748\n",
      "Early stopping, best iteration is:\n",
      "[1210]\ttraining's multi_logloss: 0.408518\tvalid_1's multi_logloss: 0.699676\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================5============================================\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[100]\ttraining's multi_logloss: 0.744354\tvalid_1's multi_logloss: 0.793504\n",
      "[200]\ttraining's multi_logloss: 0.673149\tvalid_1's multi_logloss: 0.759939\n",
      "[300]\ttraining's multi_logloss: 0.623868\tvalid_1's multi_logloss: 0.741549\n",
      "[400]\ttraining's multi_logloss: 0.585763\tvalid_1's multi_logloss: 0.729933\n",
      "[500]\ttraining's multi_logloss: 0.554152\tvalid_1's multi_logloss: 0.721836\n",
      "[600]\ttraining's multi_logloss: 0.52636\tvalid_1's multi_logloss: 0.716366\n",
      "[700]\ttraining's multi_logloss: 0.501759\tvalid_1's multi_logloss: 0.712145\n",
      "[800]\ttraining's multi_logloss: 0.47975\tvalid_1's multi_logloss: 0.709331\n",
      "[900]\ttraining's multi_logloss: 0.459436\tvalid_1's multi_logloss: 0.707728\n",
      "[1000]\ttraining's multi_logloss: 0.44102\tvalid_1's multi_logloss: 0.706947\n",
      "Early stopping, best iteration is:\n",
      "[1005]\ttraining's multi_logloss: 0.440039\tvalid_1's multi_logloss: 0.706919\n",
      "================================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "folds=[]\n",
    "for train_idx, valid_idx in skf.split(train, train['credit']):\n",
    "    folds.append((train_idx, valid_idx))\n",
    "    \n",
    "random.seed(42)\n",
    "lgb_models={}\n",
    "for fold in range(k):\n",
    "    print(f'===================================={fold+1}============================================')\n",
    "    train_idx, valid_idx = folds[fold]\n",
    "    X_train, X_valid, y_train, y_valid = train.drop(['credit'],axis=1).iloc[train_idx].values, train.drop(['credit'],axis=1).iloc[valid_idx].values,\\\n",
    "                                         train['credit'][train_idx].values, train['credit'][valid_idx].values \n",
    "    lgb = LGBMClassifier(\n",
    "        max_depth=17,\n",
    "        num_leaves=90, #90\n",
    "        min_data_in_leaf=19,  #19\n",
    "        num_iterations=1500,\n",
    "        learning_rate=0.01,\n",
    "        min_sum_hessian_in_leaf=3,\n",
    "        bagging_fraction=0.9,\n",
    "        bagging_freq=1,\n",
    "        feature_fraction=0.6,\n",
    "        min_gain_to_split=0.1,\n",
    "        lambda_l1=0.1,\n",
    "        lambda_l2=0.5,\n",
    "    )\n",
    "    lgb.fit(X_train, y_train, \n",
    "            eval_set=[(X_train, y_train), (X_valid, y_valid)], \n",
    "            early_stopping_rounds=30,\n",
    "           verbose=100)\n",
    "    lgb_models[fold]=lgb\n",
    "    print(f'================================================================================\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7020669999999999"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.687622 + 0.708098 + 0.70802 + 0.699676 + 0.706919) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7019066"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.688814 + 0.705539 + 0.708773 + 0.700425 + 0.705982) / 5 # 5-d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7016308"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.688132 + 0.708232 + 0.705494 + 0.70128 + 0.705016) / 5 # 10-d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7033164000000001"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.692293 + 0.707826 + 0.706896 + 0.700806 + 0.708761) / 5 # 15-d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7041054"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.689874 + 0.71012 + 0.708018 + 0.704542 + 0.707973) / 5# 20-d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7075440000000001"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.693812 + 0.71264 + 0.714242 + 0.704094 + .712932) / 5 # 25-d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7059782"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.692848 + 0.709351 + 0.712723 + 0.706387 + 0.708582) / 5 # 30-d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.iloc[:,1:]=0\n",
    "for fold in range(k):\n",
    "    submit.iloc[:,1:] += lgb_models[fold].predict_proba(test)/k\n",
    "    \n",
    "submit.to_csv('submission6.csv', index=False) # 0.7272812144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26457</td>\n",
       "      <td>0.063323</td>\n",
       "      <td>0.131997</td>\n",
       "      <td>0.804680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26458</td>\n",
       "      <td>0.232606</td>\n",
       "      <td>0.166730</td>\n",
       "      <td>0.600665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26459</td>\n",
       "      <td>0.035459</td>\n",
       "      <td>0.122816</td>\n",
       "      <td>0.841725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26460</td>\n",
       "      <td>0.097923</td>\n",
       "      <td>0.086686</td>\n",
       "      <td>0.815391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26461</td>\n",
       "      <td>0.104347</td>\n",
       "      <td>0.091452</td>\n",
       "      <td>0.804201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>36452</td>\n",
       "      <td>0.072683</td>\n",
       "      <td>0.262298</td>\n",
       "      <td>0.665020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>36453</td>\n",
       "      <td>0.278469</td>\n",
       "      <td>0.247625</td>\n",
       "      <td>0.473906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>36454</td>\n",
       "      <td>0.039837</td>\n",
       "      <td>0.113547</td>\n",
       "      <td>0.846615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>36455</td>\n",
       "      <td>0.159000</td>\n",
       "      <td>0.363764</td>\n",
       "      <td>0.477236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>36456</td>\n",
       "      <td>0.062447</td>\n",
       "      <td>0.200349</td>\n",
       "      <td>0.737204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index         0         1         2\n",
       "0     26457  0.063323  0.131997  0.804680\n",
       "1     26458  0.232606  0.166730  0.600665\n",
       "2     26459  0.035459  0.122816  0.841725\n",
       "3     26460  0.097923  0.086686  0.815391\n",
       "4     26461  0.104347  0.091452  0.804201\n",
       "...     ...       ...       ...       ...\n",
       "9995  36452  0.072683  0.262298  0.665020\n",
       "9996  36453  0.278469  0.247625  0.473906\n",
       "9997  36454  0.039837  0.113547  0.846615\n",
       "9998  36455  0.159000  0.363764  0.477236\n",
       "9999  36456  0.062447  0.200349  0.737204\n",
       "\n",
       "[10000 rows x 4 columns]"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('submission6.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model list\n",
    "### 0. basemodel\n",
    "\n",
    "### 1. self-tuning model / 10 fold\n",
    "\n",
    "### 2. my feature_engineering with imputation / 10 fold\n",
    "\n",
    "### 3. my feature_engineering with no imputation / 10 fold\n",
    "\n",
    "### 4. autoencode with imputation / 5 fold\n",
    "- 0.7020562734(val 0.7041054)\n",
    "\n",
    "### 5. autoencode 20-d with no imputation / 5fold\n",
    "- (val 0.7075)\n",
    "\n",
    "### 6. autoencode 10-d with no imputation / 5fold\n",
    "- 0.703402256 (val 0.70463)\n",
    "\n",
    "### 7. autoencode 9-d with imputation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Encoder\n",
    "**w / imputation**\n",
    "**기본적으로 임퓨팅하는 것이 항상 성능이 좋음**\n",
    "- 30-d : 0.7059782\n",
    "- 25-d : 0.7075440\n",
    "- 20-d : 0.7041054\n",
    "- 15-d : 0.7033164\n",
    "- 10-d : 0.7016308 ( best ) \n",
    "- 9-d : 0.7020669\n",
    "- 5-d : 0.7019066"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
